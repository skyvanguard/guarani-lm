{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Exploración de Datos GuaraniLM\n",
    "\n",
    "Este notebook explora las fuentes de datos disponibles para entrenar GuaraniLM:\n",
    "- Wikipedia Guaraní\n",
    "- CulturaX (subset grn)\n",
    "- Jojajovai (corpus paralelo gn↔es)\n",
    "- mmaguero datasets (sentiment, humor, hate speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, str(Path(\".\").resolve().parent / \"scripts\"))\n",
    "from normalize_guarani import normalize\n",
    "\n",
    "DATA_DIR = Path(\"..\") / \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wikipedia Guaraní"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_path = DATA_DIR / \"interim\" / \"wikipedia_gn.jsonl\"\n",
    "if wiki_path.exists():\n",
    "    wiki_texts = []\n",
    "    with open(wiki_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            wiki_texts.append(json.loads(line.strip()))\n",
    "    print(f\"Articles: {len(wiki_texts)}\")\n",
    "    lengths = [len(t['text'].split()) for t in wiki_texts]\n",
    "    print(f\"Total words: {sum(lengths):,}\")\n",
    "    print(f\"Mean words/article: {sum(lengths)/len(lengths):.0f}\")\n",
    "    print(f\"\\nSample article (first 200 chars):\")\n",
    "    print(wiki_texts[0]['text'][:200])\n",
    "else:\n",
    "    print(f\"File not found: {wiki_path}\")\n",
    "    print(\"Run: python scripts/download_data.py && python scripts/clean_wikipedia.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CulturaX Guaraní"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "culturax_path = DATA_DIR / \"interim\" / \"culturax_gn.jsonl\"\n",
    "if culturax_path.exists():\n",
    "    cx_texts = []\n",
    "    with open(culturax_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            cx_texts.append(json.loads(line.strip()))\n",
    "    print(f\"Documents: {len(cx_texts)}\")\n",
    "    lengths = [len(t['text'].split()) for t in cx_texts]\n",
    "    print(f\"Total words: {sum(lengths):,}\")\n",
    "    print(f\"\\nSample (first 200 chars):\")\n",
    "    print(cx_texts[0]['text'][:200])\n",
    "else:\n",
    "    print(f\"File not found: {culturax_path}\")\n",
    "    print(\"Run: python scripts/download_data.py && python scripts/clean_culturax.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Jojajovai (Parallel Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_path = DATA_DIR / \"interim\" / \"parallel_gn_es.jsonl\"\n",
    "if parallel_path.exists():\n",
    "    pairs = []\n",
    "    with open(parallel_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            pairs.append(json.loads(line.strip()))\n",
    "    print(f\"Parallel pairs: {len(pairs)}\")\n",
    "    gn_lengths = [len(p['gn'].split()) for p in pairs]\n",
    "    es_lengths = [len(p['es'].split()) for p in pairs]\n",
    "    print(f\"GN mean words: {sum(gn_lengths)/len(gn_lengths):.1f}\")\n",
    "    print(f\"ES mean words: {sum(es_lengths)/len(es_lengths):.1f}\")\n",
    "    print(f\"\\nSample pairs:\")\n",
    "    for p in pairs[:5]:\n",
    "        print(f\"  GN: {p['gn'][:80]}\")\n",
    "        print(f\"  ES: {p['es'][:80]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(f\"File not found: {parallel_path}\")\n",
    "    print(\"Run: python scripts/download_data.py && python scripts/prepare_parallel.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Character Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze character distribution in Guaraní text\n",
    "all_text = \"\"\n",
    "for path in [wiki_path, culturax_path]:\n",
    "    if path.exists():\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                all_text += json.loads(line.strip())[\"text\"] + \" \"\n",
    "\n",
    "if all_text:\n",
    "    char_counts = Counter(all_text.lower())\n",
    "    guarani_special = ['ã', 'ẽ', 'ĩ', 'õ', 'ũ', 'ỹ', 'g̃', \"'\"]\n",
    "    print(\"Guaraní-specific character frequencies:\")\n",
    "    for ch in guarani_special:\n",
    "        count = char_counts.get(ch, 0)\n",
    "        print(f\"  '{ch}': {count:,} ({count/len(all_text)*100:.3f}%)\")\n",
    "else:\n",
    "    print(\"No data loaded yet. Run the pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Statistics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "sources = {\n",
    "    \"Wikipedia GN\": DATA_DIR / \"interim\" / \"wikipedia_gn.jsonl\",\n",
    "    \"CulturaX GN\": DATA_DIR / \"interim\" / \"culturax_gn.jsonl\",\n",
    "    \"Parallel GN-ES\": DATA_DIR / \"interim\" / \"parallel_gn_es.jsonl\",\n",
    "    \"NLLB Augmented\": DATA_DIR / \"interim\" / \"augmented_nllb.jsonl\",\n",
    "}\n",
    "total_words = 0\n",
    "for name, path in sources.items():\n",
    "    if path.exists():\n",
    "        count = sum(1 for _ in open(path, encoding=\"utf-8\"))\n",
    "        print(f\"  {name}: {count:,} records\")\n",
    "    else:\n",
    "        print(f\"  {name}: NOT YET GENERATED\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
